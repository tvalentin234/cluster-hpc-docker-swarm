# cluster-hpc-docker-swarm
# Cluster HPC com Docker Swarm (2 managers + 8 workers)

Este projeto entrega **passo a passo operacional** e os artefatos (stacks, scripts e imagens) para montar um **cluster HPC** com:
- **Docker Swarm**
- **MPICH 4.1.3** (com OpenSSH) para MPI
- **Observabilidade Essencial**: Prometheus, Alertmanager, Grafana, Pushgateway
- **Exporters**: node_exporter (hosts) e cAdvisor (containers)
- **Dashboards**: *Infra Essentials* e *Encadeado compacto*
- **OrquestraÃ§Ã£o do pipeline NEWAVE/DECOMP** via Airflow (ou JS7 como alternativa)
- **PublicaÃ§Ã£o de mÃ©tricas** no Pushgateway (inÃ­cio/fim/erro/tempo)

> Documentos e materiais de referÃªncia podem ser mantidos em `docs/` (ex.: a proposta comercial do escopo).

---

## ğŸ§­ Topologia alvo

- **Managers**: 2 nÃ³s (ex.: `mgr01`, `mgr02`)
- **Workers**: 8 nÃ³s (ex.: `wrk01`â€¦`wrk08`)
- **Redes overlay**:
  - `mpi_net` â€” trÃ¡fego MPI/SSH entre *tasks* MPI
  - `svc_net` â€” serviÃ§os (observabilidade, orquestraÃ§Ã£o)

> Em ambientes de missÃ£o crÃ­tica, considere **3 managers** para maior resiliÃªncia de quÃ³rum.

---

## ğŸ“ Estrutura do repositÃ³rio

cluster-hpc-docker-swarm/
â”œâ”€ README.md
â”œâ”€ .gitignore
â”œâ”€ docs/
â”‚ â””â”€ Proposta-Comercial-Cluster-HPC-Docker-Swarm.pdf # (opcional)
â”œâ”€ scripts/
â”‚ â”œâ”€ disable-smt.sh # desativa SMT/HT (opcional)
â”‚ â”œâ”€ firewall-ufw.sh # abre portas (Ubuntu/UFW)
â”‚ â”œâ”€ firewall-firewalld.sh # abre portas (RHEL/firewalld)
â”‚ â””â”€ label-workers-compute.sh # adiciona label role=compute
â”œâ”€ stacks/
â”‚ â”œâ”€ observability.yml # Prometheus/Alertmanager/Grafana/PGW/node_exporter/cAdvisor
â”‚ â”œâ”€ mpi.yml # compute-daemon (global) + runner
â”‚ â””â”€ airflow.yml # Airflow LocalExecutor + Postgres
â”œâ”€ observability/
â”‚ â”œâ”€ prometheus/prometheus.yml
â”‚ â”œâ”€ prometheus/rules/alerts.yml
â”‚ â””â”€ alertmanager/alertmanager.yml
â”œâ”€ grafana/provisioning/
â”‚ â”œâ”€ datasources/datasource.yml # Prometheus datasource
â”‚ â””â”€ dashboards/
â”‚ â”œâ”€ provider.yml
â”‚ â””â”€ json/
â”‚ â”œâ”€ infra-essentials.json
â”‚ â””â”€ encadeado-compacto.json
â”œâ”€ mpi/
â”‚ â”œâ”€ Dockerfile # MPICH 4.1.3 + OpenSSH
â”‚ â”œâ”€ entrypoint.sh
â”‚ â”œâ”€ generate_hostfile.sh # hostfile via DNS do Swarm (tasks.compute-daemon)
â”‚ â””â”€ README.md
â””â”€ airflow/dags/
â”œâ”€ newave_decomp_dag.py # DAG exemplo
â””â”€ wrappers/
â”œâ”€ push_metric.sh # publica mÃ©tricas no Pushgateway
â”œâ”€ newave_wrapper.sh # validaÃ§Ã£o/execuÃ§Ã£o/logs (placeholder)
â””â”€ decomp_wrapper.sh # validaÃ§Ã£o/execuÃ§Ã£o/logs (placeholder)


---

## âœ… PrÃ©-requisitos (todos os nÃ³s)

- Linux (Ubuntu 24.04 LTS **ou** RHEL 10)
- Docker Engine â‰¥ 24 + Docker Compose plugin
- UsuÃ¡rio com sudo
- SincronizaÃ§Ã£o de horÃ¡rio (chrony/ntpd)
- Hostnames Ãºnicos e resoluÃ§Ã£o interna consistente (DNS ou `/etc/hosts`)
- Conectividade entre nÃ³s (veja portas ğŸ‘‡)

InstalaÃ§Ã£o rÃ¡pida do Docker (Ubuntu):

```bash
curl -fsSL https://get.docker.com | sudo bash
sudo usermod -aG docker $USER
newgrp docker

ğŸ”§ Ajustes de CPU (opcional, quando aplicÃ¡vel)

Desativar SMT/Hyper-Threading para workloads MPI mais previsÃ­veis:

TemporÃ¡rio: echo off | sudo tee /sys/devices/system/cpu/smt/control

Permanente: adicionar nosmt no GRUB e reiniciar

Pinagem/afinidade: em jobs MPI, use mpirun --bind-to core e planeje placement no Swarm com constraints e resources.

Scripts de apoio: scripts/disable-smt.sh.

| Categoria       | Portas                                                                             |
| --------------- | ---------------------------------------------------------------------------------- |
| Docker Swarm    | **2377/TCP** (mgmt), **7946/TCP+UDP**, **4789/UDP**                                |
| SSH             | **22/TCP**                                                                         |
| ServiÃ§os (apps) | **6000â€“9000/TCP** (ajuste conforme app)                                            |
| Observabilidade | **9090** Prometheus, **9091** Pushgateway, **9093** Alertmanager, **3000** Grafana |


Scripts prontos:

Ubuntu/UFW: scripts/firewall-ufw.sh

ğŸ³ Swarm: init, quÃ³rum, labels
1) Inicializar Swarm no manager principal
# no mgr01
docker swarm init --advertise-addr <IP_MGR01>

# tokens
docker swarm join-token manager
docker swarm join-token worker

2) Entrar com o 2Âº manager e os 8 workers
# mgr02 (como manager)
docker swarm join --token <TOKEN_MANAGER> <IP_MGR01>:2377

# workers (cada um)
docker swarm join --token <TOKEN_WORKER> <IP_MGR01>:2377

Verifique: docker node ls (deve listar 10 nÃ³s).
Com 2 managers, o quÃ³rum Ã© 2 (Leader + Reachable). Para maior resiliÃªncia, considere 3 managers.

3) Labels e roles (workers como compute)
# em um manager
./scripts/label-workers-compute.sh
# ou manual:
# docker node update --label-add role=compute wrk01
# ...

ğŸŒ Redes overlay

Crie as redes (em um manager):

docker network create -d overlay --attachable mpi_net
docker network create -d overlay --attachable svc_net

ğŸ“Š Observabilidade Essencial

Suba a stack:

docker stack deploy -c stacks/observability.yml observ


Acessos padrÃ£o:

Prometheus: http://<manager>:9090

Alertmanager: http://<manager>:9093

Pushgateway: http://<manager>:9091

Grafana: http://<manager>:3000 (login inicial admin/admin â†’ troque)

Dashboards provisionados:

Infra Essentials

Encadeado compacto

Alertas essenciais (exemplos, em observability/prometheus/rules/alerts.yml):

Host/serviÃ§o indisponÃ­vel (up == 0)

CPU, memÃ³ria e disco altos

Pipeline stalled (sem push de mÃ©tricas por tempo X)

Alertmanager: configure destinatÃ¡rios (email/Slack/webhook) em observability/alertmanager/alertmanager.yml.

ğŸ§ª Runtime MPI (MPICH 4.1.3)
Imagem MPICH com OpenSSH

Edite a referÃªncia do registry e faÃ§a build/push:

# ajuste REGISTRY (ex.: registry.local, ghcr.io/owner, etc.)
export REGISTRY="registry.local"
docker build -t $REGISTRY/mpi-mpich:4.1.3 ./mpi
docker push $REGISTRY/mpi-mpich:4.1.3

Daemon global nos workers e runner
docker stack deploy -c stacks/mpi.yml mpi


compute-daemon roda global nos nÃ³s com role=compute e mantÃ©m sshd disponÃ­vel.

mpi-runner (no manager) gera /etc/mpi/hostfile dinamicamente via DNS do Swarm (tasks.compute-daemon) usando mpi/generate_hostfile.sh.

Teste rÃ¡pido:

# entrar no runner (ou executar um comando via service)
docker ps | grep mpi-runner
docker exec -it <container_mpi-runner> bash

# dentro do container:
generate_hostfile
cat /etc/mpi/hostfile
mpirun -f /etc/mpi/hostfile -np 8 --bind-to core hostname


Ajuste slots, binding e mapping (--map-by, --bind-to) conforme a topologia de CPU.

â›“ï¸ Encadeado NEWAVE/DECOMP (Airflow ou JS7)
Airflow (LocalExecutor)
docker stack deploy -c stacks/airflow.yml airflow


UI: http://<manager>:8080 (admin/admin na criaÃ§Ã£o)

DAG exemplo: airflow/dags/newave_decomp_dag.py

Wrappers: airflow/dags/wrappers/ publicam mÃ©tricas no Pushgateway (push_metric.sh)

Personalize:

ValidaÃ§Ã£o de decks, timeouts, coleta de logs (incluir comandos reais do NEWAVE/DECOMP nos wrappers)

Schedules e dependÃªncias do DAG

Roteiros de incidentes no Alertmanager

Alternativa: JS7 (Open Source)

Estruture jobs/orders anÃ¡logos aos tasks do Airflow e reutilize os wrappers para mÃ©tricas.

Mantenha o mesmo Pushgateway e dashboards.

ğŸ”’ SeguranÃ§a (boas prÃ¡ticas)

Acesse observabilidade apenas via VPN / IPs permitidos (firewall/ingress)

Use Docker Swarm Secrets para credenciais

Considere docker swarm update --autolock=true

FaÃ§a patching regular de SO/Kernel e Engine Docker

Limite o escopo de portas 6000â€“9000/TCP aos serviÃ§os que precisarem

ğŸ§° Troubleshooting rÃ¡pido

docker node ls mostra nÃ³ Down/Unknown
Verifique rede/firewall, /etc/hosts, DNS e hora (NTP).

Falha ao criar overlay (ingress/VXLAN)
Libere UDP 4789 e TCP/UDP 7946 entre nÃ³s; confirme que nÃ£o hÃ¡ conflitos de MTU.

Grafana sem dashboards
Cheque grafana/provisioning/... montado e logs do container.

Alertas nÃ£o chegam
Configure alertmanager.yml com receiver real (email/Slack/webhook) e reinicie.

tasks.compute-daemon nÃ£o resolve
O serviÃ§o compute-daemon precisa estar em mode: global e as redes criadas; confira docker service ps e DNS do Swarm.

ğŸ“š Handover

Runbook: este README + pastas stacks/, scripts/, observability/, airflow/, mpi/
